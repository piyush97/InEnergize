# InErgize Backup and Disaster Recovery Strategy
# Comprehensive data protection and business continuity plan

# Global backup configuration
global_config:
  backup_retention:
    daily: 30    # Keep daily backups for 30 days
    weekly: 12   # Keep weekly backups for 12 weeks
    monthly: 12  # Keep monthly backups for 12 months
    yearly: 7    # Keep yearly backups for 7 years
  
  encryption:
    algorithm: "AES-256-GCM"
    key_rotation_days: 90
    key_escrow: true
  
  compression:
    enabled: true
    algorithm: "zstd"  # High compression ratio with good speed
    level: 6
  
  verification:
    enabled: true
    schedule: "daily"
    integrity_checks: true
  
  monitoring:
    alerts_enabled: true
    failure_threshold: 1
    success_notification: false
    channels: ["email", "slack", "pagerduty"]

# Database backup strategies
database_backups:
  postgresql_primary:
    type: "postgresql"
    source:
      host: "inergize-postgres"
      port: 5432
      database: "inergize_dev"
      user: "inergize_user"
    
    backup_methods:
      # Continuous WAL archiving for PITR
      wal_archiving:
        enabled: true
        archive_command: "gzip < %p > /backup/wal/%f.gz"
        archive_timeout: "60s"
        max_wal_size: "2GB"
        retention: "7d"
      
      # Daily full backups
      full_backup:
        schedule: "0 2 * * *"  # 2 AM daily
        method: "pg_dump"
        format: "custom"
        compression: true
        parallel_jobs: 4
        includes:
          - "users"
          - "profiles" 
          - "linkedin_data"
          - "automation_history"
          - "audit_logs"
        excludes:
          - "temp_*"
          - "cache_*"
      
      # Incremental backups
      incremental_backup:
        schedule: "0 */6 * * *"  # Every 6 hours
        method: "pg_basebackup"
        format: "tar"
        compression: true
    
    recovery:
      rpo: "15m"  # Recovery Point Objective: max 15 minutes data loss
      rto: "1h"   # Recovery Time Objective: max 1 hour downtime
      automated_failover: true
      standby_servers: 2
      
  timescaledb_analytics:
    type: "timescaledb"
    source:
      host: "inergize-timescale"
      port: 5432
      database: "inergize_analytics"
      user: "inergize_user"
    
    backup_methods:
      # TimescaleDB specific continuous aggregate backups
      continuous_aggregates:
        schedule: "0 1 * * *"  # 1 AM daily
        retention_policy: "30d"
        compression: true
      
      # Raw time-series data backup
      timeseries_backup:
        schedule: "0 3 * * *"  # 3 AM daily
        chunk_compression: true
        parallel_workers: 8
        retention: "90d"  # Analytics data retention
    
    recovery:
      rpo: "1h"   # Analytics can tolerate more data loss
      rto: "4h"   # Analytics recovery is less critical
      automated_failover: false

# Redis backup configuration
redis_backups:
  cache_and_sessions:
    type: "redis"
    source:
      host: "inergize-redis"
      port: 6379
      databases: [0, 1, 2, 3, 4, 5]  # All Redis databases
    
    backup_methods:
      # RDB snapshots
      rdb_snapshot:
        schedule: "0 */4 * * *"  # Every 4 hours
        save_config: "900 1 300 10 60 10000"  # Redis save configuration
        compression: true
      
      # AOF (Append Only File) backup
      aof_backup:
        enabled: true
        fsync: "everysec"  # Fsync every second
        auto_rewrite: true
        rewrite_threshold: 100
    
    recovery:
      rpo: "1h"   # Sessions can be recreated
      rto: "30m"  # Fast recovery for cache
      automated_failover: true
      replica_servers: 1

# File system backups
filesystem_backups:
  application_data:
    type: "filesystem"
    sources:
      - "/app/uploads"
      - "/app/logs"
      - "/app/config"
      - "/etc/ssl"
      - "/var/lib/docker/volumes"
    
    backup_methods:
      incremental_sync:
        schedule: "0 */2 * * *"  # Every 2 hours
        method: "rsync"
        options: "--archive --compress --delete --hard-links"
        exclude_patterns:
          - "*.tmp"
          - "*.log"
          - "node_modules/"
          - ".git/"
      
      snapshot_backup:
        schedule: "0 4 * * *"  # 4 AM daily
        method: "tar"
        compression: "zstd"
        level: 6
    
    recovery:
      rpo: "2h"
      rto: "1h"

  secrets_backup:
    type: "encrypted_filesystem"
    sources:
      - "/app/.secrets"
      - "/etc/ssl/private"
    
    backup_methods:
      encrypted_archive:
        schedule: "0 5 * * *"  # 5 AM daily
        encryption: "GPG"
        recipients: ["backup@inergize.com", "security@inergize.com"]
        compression: "xz"
    
    recovery:
      rpo: "24h"  # Secrets change infrequently
      rto: "30m"
      manual_approval_required: true

# Cloud storage configuration
storage_targets:
  aws_s3:
    primary: true
    bucket: "inergize-backups-primary"
    region: "us-east-1"
    storage_class: "STANDARD_IA"  # Infrequent Access
    lifecycle_rules:
      - transition_to_glacier: "30d"
      - transition_to_deep_archive: "90d"
      - expire_after: "7y"
    
    encryption:
      type: "SSE-KMS"
      key_id: "arn:aws:kms:us-east-1:account:key/backup-key"
      
    cross_region_replication:
      enabled: true
      destination_bucket: "inergize-backups-replica"
      destination_region: "us-west-2"
  
  azure_blob:
    secondary: true
    container: "inergize-backups-secondary"
    storage_account: "inergizebackups"
    access_tier: "Cool"
    
    retention_policy:
      days: 2555  # 7 years
      
  local_storage:
    emergency: true
    path: "/backup/local"
    capacity: "2TB"
    rotation: "7d"  # Local backups for quick recovery

# Disaster recovery procedures
disaster_recovery:
  scenarios:
    database_corruption:
      detection:
        - "Data integrity check failures"
        - "Replication lag > 1 hour"
        - "Query failures > 5%"
      
      response_steps:
        1: "Stop application writes"
        2: "Assess corruption scope"
        3: "Restore from latest clean backup"
        4: "Apply WAL logs for PITR"
        5: "Verify data integrity"
        6: "Resume application"
      
      estimated_rto: "2h"
      estimated_rpo: "15m"
    
    complete_infrastructure_loss:
      detection:
        - "All services unreachable"
        - "Infrastructure monitoring failure"
        - "Physical/cloud provider issues"
      
      response_steps:
        1: "Activate disaster recovery site"
        2: "Deploy infrastructure from IaC"
        3: "Restore databases from backups"
        4: "Restore application data"
        5: "Update DNS records"
        6: "Verify all systems operational"
      
      estimated_rto: "8h"
      estimated_rpo: "1h"
    
    security_breach:
      detection:
        - "Unauthorized access detected"
        - "Data exfiltration alerts"
        - "Malware/ransomware detected"
      
      response_steps:
        1: "Isolate affected systems"
        2: "Preserve forensic evidence"
        3: "Restore from clean backups"
        4: "Rotate all credentials"
        5: "Security audit and hardening"
        6: "Gradual service restoration"
      
      estimated_rto: "12h"
      estimated_rpo: "4h"
    
    linkedin_api_suspension:
      detection:
        - "LinkedIn API 403/429 errors"
        - "Account suspension notifications"
        - "Compliance violations detected"
      
      response_steps:
        1: "Halt all LinkedIn automation"
        2: "Preserve audit logs"
        3: "Review compliance violations"
        4: "Prepare appeal documentation"
        5: "Implement additional safeguards"
        6: "Request API access restoration"
      
      estimated_rto: "24-72h"  # External dependency
      estimated_rpo: "0"  # No data loss

# Business continuity planning
business_continuity:
  critical_services:
    - name: "User Authentication"
      priority: 1
      dependencies: ["postgres", "redis"]
      fallback: "Read-only mode with cached sessions"
    
    - name: "LinkedIn Integration"
      priority: 2
      dependencies: ["postgres", "redis", "linkedin-api"]
      fallback: "Queue requests for later processing"
    
    - name: "Analytics Dashboard"
      priority: 3
      dependencies: ["timescaledb", "redis"]
      fallback: "Show cached data with staleness warnings"
    
    - name: "AI Content Generation"
      priority: 4
      dependencies: ["openai-api", "postgres"]
      fallback: "Use cached content templates"
  
  communication_plan:
    internal:
      - "Slack #incidents channel"
      - "Email distribution list"
      - "PagerDuty escalation"
    
    external:
      - "Status page updates"
      - "Customer email notifications"
      - "Social media announcements"
  
  staff_assignments:
    incident_commander: "CTO"
    technical_lead: "Senior DevOps Engineer"
    communications_lead: "Customer Success Manager"
    security_lead: "Security Engineer"

# Backup automation and monitoring
automation:
  backup_orchestrator:
    image: "inergize/backup-orchestrator:latest"
    schedule_engine: "cron"
    parallel_jobs: 4
    timeout: "4h"
    
    health_checks:
      - "Database connectivity"
      - "Storage availability"
      - "Encryption key access"
      - "Network bandwidth"
    
    failure_handling:
      max_retries: 3
      retry_delay: "30m"
      escalation_threshold: 2
      
  monitoring_integration:
    prometheus_metrics:
      - "backup_duration_seconds"
      - "backup_size_bytes"
      - "backup_success_total"
      - "backup_failure_total"
      - "restore_test_success"
    
    grafana_dashboard: "InErgize Backup & Recovery"
    
    alerting_rules:
      - name: "backup_failure"
        condition: "backup_success_total == 0 for 25h"
        severity: "critical"
      
      - name: "backup_size_anomaly"
        condition: "backup_size_bytes > 2 * avg_over_time(backup_size_bytes[7d])"
        severity: "warning"
      
      - name: "restore_test_failure"
        condition: "restore_test_success == 0 for 7d"
        severity: "high"

# Recovery testing
recovery_testing:
  schedule:
    full_disaster_recovery: "quarterly"
    database_restore: "monthly"
    file_restore: "weekly"
    
  test_scenarios:
    - "Point-in-time recovery to specific timestamp"
    - "Cross-region disaster recovery"
    - "Partial data corruption recovery"
    - "Complete infrastructure rebuild"
    - "LinkedIn compliance data restoration"
  
  success_criteria:
    - "RTO targets met"
    - "RPO targets met"
    - "Data integrity verified"
    - "All critical services operational"
    - "Performance within acceptable ranges"
  
  documentation:
    runbooks: "/docs/disaster-recovery/"
    test_reports: "/reports/recovery-testing/"
    lessons_learned: "/docs/post-mortem/"

# Compliance and legal requirements
compliance:
  data_retention:
    user_data: "7 years (GDPR Article 5)"
    audit_logs: "10 years (SOX compliance)"
    linkedin_data: "2 years (LinkedIn ToS)"
    financial_records: "7 years (tax requirements)"
  
  geographical_requirements:
    eu_data: "Must remain in EU (GDPR)"
    us_data: "Can be stored globally"
    encrypted_data: "Global storage permitted"
  
  regulatory_compliance:
    gdpr: "Right to be forgotten implemented"
    sox: "Audit trail preservation"
    pci_dss: "Secure backup of payment data"
    iso27001: "Information security management"

# Cost optimization
cost_optimization:
  storage_tiering:
    hot: "Recent backups (7 days) - Standard storage"
    warm: "Weekly backups (30 days) - Infrequent Access"
    cold: "Monthly backups (12 months) - Glacier"
    archive: "Yearly backups (7 years) - Deep Archive"
  
  compression_targets:
    database_backups: "70% size reduction"
    log_files: "90% size reduction"
    file_system: "50% size reduction"
  
  deduplication:
    enabled: true
    method: "block-level"
    savings_target: "30%"
  
  lifecycle_management:
    automatic_deletion: true
    policy_enforcement: true
    cost_monitoring: true