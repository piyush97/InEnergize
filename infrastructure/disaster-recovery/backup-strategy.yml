# InErgize Production Disaster Recovery and Backup Strategy
# Multi-region backup with automated recovery procedures

---
# Velero Backup Configuration
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: primary-backup-s3
  namespace: velero
  labels:
    environment: production
    purpose: disaster-recovery
spec:
  provider: aws
  objectStorage:
    bucket: inergize-production-backups
    prefix: kubernetes
  config:
    region: us-east-1
    s3ForcePathStyle: "false"
    kmsKeyId: "arn:aws:kms:us-east-1:ACCOUNT:key/backup-key-id"
  credential:
    name: cloud-credentials
    key: cloud

---
# Secondary Backup Location (Cross-region)
apiVersion: velero.io/v1
kind: BackupStorageLocation
metadata:
  name: secondary-backup-s3
  namespace: velero
  labels:
    environment: production
    purpose: cross-region-dr
spec:
  provider: aws
  objectStorage:
    bucket: inergize-dr-backups-west
    prefix: kubernetes
  config:
    region: us-west-2
    s3ForcePathStyle: "false"
    kmsKeyId: "arn:aws:kms:us-west-2:ACCOUNT:key/backup-key-id-west"
  credential:
    name: cloud-credentials
    key: cloud

---
# Volume Snapshot Location
apiVersion: velero.io/v1
kind: VolumeSnapshotLocation
metadata:
  name: primary-volume-snapshots
  namespace: velero
  labels:
    environment: production
spec:
  provider: aws
  config:
    region: us-east-1
    enableSharedSnapshots: "true"
  credential:
    name: cloud-credentials
    key: cloud

---
# Daily Production Backup
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-production-backup
  namespace: velero
  labels:
    environment: production
    backup-type: daily
spec:
  schedule: "0 2 * * *"  # 2 AM EST daily
  template:
    metadata:
      labels:
        backup-type: daily
        environment: production
    spec:
      storageLocation: primary-backup-s3
      volumeSnapshotLocations:
        - primary-volume-snapshots
      includedNamespaces:
        - inergize-production
        - inergize-monitoring
      includedResources:
        - '*'
      excludedResources:
        - events
        - events.events.k8s.io
      labelSelector:
        matchLabels:
          backup.enabled: "true"
      snapshotVolumes: true
      defaultVolumesToRestic: false
      ttl: 720h  # 30 days retention
      hooks:
        resources:
          - name: postgres-backup-hook
            includedNamespaces:
              - inergize-production
            includedResources:
              - pods
            labelSelector:
              matchLabels:
                app: postgresql
            pre:
              - exec:
                  container: postgresql
                  command:
                    - /bin/bash
                    - -c
                    - |
                      echo "Starting PostgreSQL backup preparation..."
                      pg_dump -h localhost -U $POSTGRES_USER -d $POSTGRES_DB --no-password > /tmp/backup.sql
                      echo "PostgreSQL backup preparation complete"
                  onError: Fail
                  timeout: 300s
            post:
              - exec:
                  container: postgresql
                  command:
                    - /bin/bash
                    - -c
                    - |
                      echo "Cleaning up PostgreSQL backup files..."
                      rm -f /tmp/backup.sql
                      echo "PostgreSQL backup cleanup complete"
                  onError: Continue
                  timeout: 60s
          
          - name: timescale-backup-hook
            includedNamespaces:
              - inergize-production
            includedResources:
              - pods
            labelSelector:
              matchLabels:
                app: timescaledb
            pre:
              - exec:
                  container: timescaledb
                  command:
                    - /bin/bash
                    - -c
                    - |
                      echo "Starting TimescaleDB backup preparation..."
                      pg_dump -h localhost -U $POSTGRES_USER -d $POSTGRES_DB --no-password --format=custom > /tmp/timescale_backup.dump
                      echo "TimescaleDB backup preparation complete"
                  onError: Fail
                  timeout: 600s
            post:
              - exec:
                  container: timescaledb
                  command:
                    - /bin/bash
                    - -c
                    - |
                      echo "Cleaning up TimescaleDB backup files..."
                      rm -f /tmp/timescale_backup.dump
                      echo "TimescaleDB backup cleanup complete"
                  onError: Continue
                  timeout: 60s

---
# Hourly Critical Data Backup (LinkedIn compliance data)
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: hourly-critical-backup
  namespace: velero
  labels:
    environment: production
    backup-type: critical
    compliance: required
spec:
  schedule: "0 * * * *"  # Every hour
  template:
    metadata:
      labels:
        backup-type: critical
        environment: production
        compliance: required
    spec:
      storageLocation: primary-backup-s3
      volumeSnapshotLocations:
        - primary-volume-snapshots
      includedNamespaces:
        - inergize-production
      labelSelector:
        matchLabels:
          app: linkedin-service
      orLabelSelectors:
        - matchLabels:
            component: compliance
        - matchLabels:
            data-classification: critical
      snapshotVolumes: true
      ttl: 168h  # 7 days retention for critical data
      hooks:
        resources:
          - name: linkedin-compliance-data-backup
            includedNamespaces:
              - inergize-production
            includedResources:
              - pods
            labelSelector:
              matchLabels:
                app: linkedin-service
            pre:
              - exec:
                  container: linkedin-service
                  command:
                    - /bin/bash
                    - -c
                    - |
                      echo "Backing up LinkedIn compliance data..."
                      # Export critical compliance metrics
                      redis-cli -h redis -a $REDIS_PASSWORD --json GET compliance:linkedin:health_score > /tmp/compliance_backup.json
                      redis-cli -h redis -a $REDIS_PASSWORD --json GET compliance:linkedin:api_calls > /tmp/api_calls_backup.json
                      echo "LinkedIn compliance data backup complete"
                  onError: Fail
                  timeout: 120s

---
# Weekly Cross-Region Backup
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: weekly-cross-region-backup
  namespace: velero
  labels:
    environment: production
    backup-type: cross-region
spec:
  schedule: "0 1 * * 0"  # 1 AM EST every Sunday
  template:
    metadata:
      labels:
        backup-type: cross-region
        environment: production
    spec:
      storageLocation: secondary-backup-s3
      includedNamespaces:
        - inergize-production
        - inergize-monitoring
      includedResources:
        - '*'
      snapshotVolumes: true
      ttl: 2160h  # 90 days retention
      
---
# Automated Disaster Recovery Plan
apiVersion: v1
kind: ConfigMap
metadata:
  name: disaster-recovery-plan
  namespace: inergize-production
  labels:
    component: disaster-recovery
    environment: production
data:
  recovery-procedures.md: |
    # InErgize Disaster Recovery Procedures
    
    ## RTO/RPO Targets
    - **RTO (Recovery Time Objective)**: 4 hours
    - **RPO (Recovery Point Objective)**: 1 hour
    - **Critical Services RTO**: 30 minutes
    - **LinkedIn Compliance RTO**: 15 minutes
    
    ## Disaster Scenarios and Response
    
    ### 1. Complete Cluster Failure
    **Detection**: Cluster unreachable, all services down
    **Response Time**: Immediate (< 5 minutes)
    
    ```bash
    # Step 1: Verify backup availability
    velero backup get --selector backup-type=daily
    
    # Step 2: Provision new cluster
    terraform apply -var="cluster_name=inergize-dr-$(date +%s)"
    
    # Step 3: Install Velero in new cluster
    helm install velero vmware-tanzu/velero \
      --namespace velero \
      --create-namespace \
      -f disaster-recovery/velero-values.yml
    
    # Step 4: Restore from latest backup
    velero restore create dr-restore-$(date +%s) \
      --from-backup daily-production-backup-$(date +%Y%m%d)
    
    # Step 5: Update DNS to point to new cluster
    kubectl apply -f disaster-recovery/dns-failover.yml
    
    # Step 6: Verify LinkedIn compliance service
    kubectl get pods -n inergize-production -l app=linkedin-service
    ```
    
    ### 2. Database Corruption/Loss
    **Detection**: Database connection failures, data inconsistency
    **Response Time**: < 1 hour
    
    ```bash
    # Step 1: Stop all services writing to database
    kubectl scale deployment --replicas=0 -n inergize-production \
      -l component=application
    
    # Step 2: Restore database from snapshot
    aws rds restore-db-instance-from-db-snapshot \
      --db-instance-identifier inergize-production-restored \
      --db-snapshot-identifier latest-automated-snapshot
    
    # Step 3: Update database connection strings
    kubectl patch secret database-secrets -n inergize-production \
      --patch='{"data":{"postgres-host":"aW5lcmdpemUtcHJvZHVjdGlvbi1yZXN0b3JlZA=="}}'
    
    # Step 4: Restart services
    kubectl rollout restart deployment -n inergize-production
    ```
    
    ### 3. LinkedIn Service Failure
    **Detection**: LinkedIn compliance alerts, service health checks failing
    **Response Time**: < 15 minutes
    **Priority**: CRITICAL (compliance risk)
    
    ```bash
    # Step 1: Emergency stop all LinkedIn automation
    kubectl patch deployment linkedin-service -n inergize-production \
      --patch='{"spec":{"replicas":0}}'
    
    # Step 2: Restore from hourly critical backup
    velero restore create linkedin-emergency-restore-$(date +%s) \
      --from-backup hourly-critical-backup-$(date +%Y%m%d%H) \
      --include-resources pods,secrets,configmaps \
      --selector app=linkedin-service
    
    # Step 3: Verify compliance data integrity
    kubectl exec -n inergize-production deployment/linkedin-service -- \
      /app/scripts/verify-compliance-data.sh
    
    # Step 4: Gradual service restart with monitoring
    kubectl scale deployment linkedin-service -n inergize-production --replicas=1
    # Wait for health check, then scale to 2
    ```
    
    ### 4. Cross-Region Failover
    **Scenario**: Primary region unavailable
    **Response Time**: < 2 hours
    
    ```bash
    # Step 1: Activate secondary region cluster
    kubectl config use-context inergize-dr-west
    
    # Step 2: Restore from cross-region backup
    velero restore create cross-region-failover-$(date +%s) \
      --from-backup weekly-cross-region-backup-latest
    
    # Step 3: Update DNS and load balancer configuration
    aws route53 change-resource-record-sets \
      --hosted-zone-id Z123456789 \
      --change-batch file://disaster-recovery/dns-failover-west.json
    
    # Step 4: Scale up services
    kubectl apply -f disaster-recovery/west-region-scaling.yml
    ```
    
    ## Recovery Validation Checklist
    
    ### Critical Services Health Check
    - [ ] Auth Service responding (< 200ms)
    - [ ] LinkedIn Service healthy (compliance score > 80)
    - [ ] Analytics Service processing data
    - [ ] AI Service generating content
    - [ ] User Service accessible
    - [ ] All databases accessible and consistent
    
    ### Compliance Validation
    - [ ] LinkedIn automation patterns within limits
    - [ ] No rate limit violations in last 24h
    - [ ] Account status = Active (1)
    - [ ] Health score > 80
    - [ ] All compliance monitoring active
    
    ### Data Integrity Verification
    ```bash
    # PostgreSQL consistency check
    kubectl exec -n inergize-production postgresql-0 -- \
      psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT COUNT(*) FROM users;"
    
    # TimescaleDB hypertable verification
    kubectl exec -n inergize-production timescaledb-0 -- \
      psql -U $POSTGRES_USER -d $POSTGRES_DB -c "SELECT * FROM timescaledb_information.hypertables;"
    
    # Redis cluster health
    kubectl exec -n inergize-production redis-0 -- redis-cli cluster info
    ```
    
    ## Rollback Procedures
    
    ### Automated Rollback Triggers
    - Service health checks failing for > 5 minutes
    - LinkedIn compliance score < 60 for > 2 minutes
    - Error rate > 10% for > 3 minutes
    
    ```bash
    # Emergency rollback to previous backup
    velero restore create emergency-rollback-$(date +%s) \
      --from-backup $(velero backup get --selector backup-type=daily | grep Completed | head -n2 | tail -n1 | awk '{print $1}')
    ```
  
  recovery-scripts.sh: |
    #!/bin/bash
    # Automated recovery scripts for common scenarios
    
    set -euo pipefail
    
    # Configuration
    NAMESPACE="inergize-production"
    BACKUP_BUCKET="inergize-production-backups"
    DR_BUCKET="inergize-dr-backups-west"
    SLACK_WEBHOOK="${SLACK_WEBHOOK_URL}"
    
    # Function to send notifications
    notify_slack() {
        local message="$1"
        local color="${2:-warning}"
        curl -X POST "$SLACK_WEBHOOK" \
            -H 'Content-type: application/json' \
            --data "{\"attachments\":[{\"color\":\"$color\",\"text\":\"$message\"}]}"
    }
    
    # Function to check service health
    check_service_health() {
        local service="$1"
        local endpoint="/health"
        local timeout=30
        
        if kubectl exec -n "$NAMESPACE" "deployment/$service" -- \
           curl -f "http://localhost:3001$endpoint" --max-time "$timeout" > /dev/null 2>&1; then
            echo "‚úÖ $service is healthy"
            return 0
        else
            echo "‚ùå $service is unhealthy"
            return 1
        fi
    }
    
    # Function to restore from backup
    restore_from_backup() {
        local backup_name="$1"
        local restore_name="auto-restore-$(date +%s)"
        
        echo "Starting restore from backup: $backup_name"
        velero restore create "$restore_name" --from-backup "$backup_name"
        
        # Wait for restore to complete
        while true; do
            status=$(velero restore get "$restore_name" -o jsonpath='{.status.phase}')
            if [[ "$status" == "Completed" ]]; then
                echo "‚úÖ Restore completed successfully"
                break
            elif [[ "$status" == "Failed" ]]; then
                echo "‚ùå Restore failed"
                exit 1
            fi
            sleep 10
        done
    }
    
    # Main recovery function
    main() {
        case "${1:-}" in
            "health-check")
                echo "Performing comprehensive health check..."
                failed_services=()
                
                for service in auth-service linkedin-service analytics-service ai-service user-service; do
                    if ! check_service_health "$service"; then
                        failed_services+=("$service")
                    fi
                done
                
                if [[ ${#failed_services[@]} -gt 0 ]]; then
                    notify_slack "üö® Health check failed for services: ${failed_services[*]}" "danger"
                    exit 1
                else
                    notify_slack "‚úÖ All services healthy" "good"
                fi
                ;;
            
            "emergency-restore")
                echo "Performing emergency restore..."
                latest_backup=$(velero backup get --selector backup-type=daily --output json | \
                    jq -r '.items[] | select(.status.phase=="Completed") | .metadata.name' | \
                    head -n1)
                
                if [[ -n "$latest_backup" ]]; then
                    notify_slack "üîÑ Starting emergency restore from: $latest_backup" "warning"
                    restore_from_backup "$latest_backup"
                    notify_slack "‚úÖ Emergency restore completed" "good"
                else
                    notify_slack "‚ùå No valid backup found for emergency restore" "danger"
                    exit 1
                fi
                ;;
            
            "linkedin-emergency")
                echo "LinkedIn service emergency recovery..."
                
                # Stop all LinkedIn automation immediately
                kubectl scale deployment linkedin-service -n "$NAMESPACE" --replicas=0
                notify_slack "üõë LinkedIn service stopped for emergency recovery" "warning"
                
                # Restore from hourly critical backup
                latest_critical=$(velero backup get --selector backup-type=critical --output json | \
                    jq -r '.items[] | select(.status.phase=="Completed") | .metadata.name' | \
                    head -n1)
                
                if [[ -n "$latest_critical" ]]; then
                    restore_from_backup "$latest_critical"
                    kubectl scale deployment linkedin-service -n "$NAMESPACE" --replicas=1
                    notify_slack "‚úÖ LinkedIn service restored from critical backup" "good"
                else
                    notify_slack "‚ùå LinkedIn critical backup restore failed" "danger"
                    exit 1
                fi
                ;;
            
            *)
                echo "Usage: $0 {health-check|emergency-restore|linkedin-emergency}"
                exit 1
                ;;
        esac
    }
    
    main "$@"

---
# Disaster Recovery CronJob for Health Monitoring
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dr-health-monitor
  namespace: inergize-production
  labels:
    component: disaster-recovery
    purpose: health-monitoring
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: dr-health-monitor
        spec:
          restartPolicy: OnFailure
          securityContext:
            runAsNonRoot: true
            runAsUser: 1000
            fsGroup: 1000
          containers:
            - name: health-monitor
              image: ghcr.io/inergize/disaster-recovery:latest
              imagePullPolicy: Always
              env:
                - name: NAMESPACE
                  value: "inergize-production"
                - name: SLACK_WEBHOOK_URL
                  valueFrom:
                    secretKeyRef:
                      name: monitoring-secrets
                      key: alertmanager-webhook
              command:
                - /bin/bash
                - -c
                - |
                  source /app/recovery-scripts.sh
                  main health-check
              resources:
                requests:
                  memory: "128Mi"
                  cpu: "100m"
                limits:
                  memory: "256Mi"
                  cpu: "200m"
              securityContext:
                allowPrivilegeEscalation: false
                readOnlyRootFilesystem: true
                runAsNonRoot: true
                capabilities:
                  drop:
                    - ALL
              volumeMounts:
                - name: scripts
                  mountPath: /app
                  readOnly: true
          volumes:
            - name: scripts
              configMap:
                name: disaster-recovery-plan
                defaultMode: 0755